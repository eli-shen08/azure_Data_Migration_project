{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "statement_id": -1,
              "statement_ids": [],
              "state": "session_error",
              "livy_statement_state": null,
              "spark_jobs": null,
              "session_id": null,
              "normalized_state": "session_error",
              "queued_time": "2025-11-03T22:06:41.8947028Z",
              "session_start_time": "2025-11-03T22:06:41.8962688Z",
              "execution_start_time": null,
              "execution_finish_time": null,
              "parent_msg_id": "a7664faf-ba9c-4720-ad9c-84c447c5fd77"
            },
            "text/plain": "StatementMeta(, , -1, SessionError, , SessionError)"
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "AVAILABLE_WORKSPACE_CAPACITY_EXCEEDED",
          "evalue": "Livy session has failed. Session state: Error. Error code: AVAILABLE_WORKSPACE_CAPACITY_EXCEEDED. Your job requested 12 vcores. However, the workspace only has 0 vcores available out of quota of 12 vcores for node size family [MemoryOptimized]. Try ending the running job(s), reducing the numbers of vcores requested or increasing your vcore quota. https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-concepts#workspace-level Source: User.",
          "traceback": [
            "AVAILABLE_WORKSPACE_CAPACITY_EXCEEDED: Livy session has failed. Session state: Error. Error code: AVAILABLE_WORKSPACE_CAPACITY_EXCEEDED. Your job requested 12 vcores. However, the workspace only has 0 vcores available out of quota of 12 vcores for node size family [MemoryOptimized]. Try ending the running job(s), reducing the numbers of vcores requested or increasing your vcore quota. https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-concepts#workspace-level Source: User."
          ]
        }
      ],
      "metadata": {},
      "source": [
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Define paths\n",
        "base_path = \"abfss://migration@hierarchstorage25.dfs.core.windows.net/bronze/migration/\"\n",
        "output_base_path = \"abfss://migration@hierarchstorage25.dfs.core.windows.net/silver/migration/\"\n",
        "\n",
        "# Enable Delta Lake optimizations\n",
        "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
        "spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\")\n",
        "\n",
        "def validate_data_quality(df, table_name):\n",
        "    \"\"\"Validate data quality and log results.\"\"\"\n",
        "    total_records = df.count()\n",
        "    null_records = df.filter(col(\"CustomerID\").isNull()).count() if \"CustomerID\" in df.columns else 0\n",
        "    \n",
        "    quality_score = ((total_records - null_records) / total_records * 100) if total_records > 0 else 0\n",
        "    \n",
        "    logger.info(f\"{table_name} - Total records: {total_records}, Quality score: {quality_score:.2f}%\")\n",
        "    \n",
        "    if quality_score < 95:\n",
        "        logger.warning(f\"Data quality below threshold for {table_name}\")\n",
        "    \n",
        "    return quality_score\n",
        "\n",
        "def clean_and_validate_customers(df):\n",
        "    \"\"\"Clean and validate customer data with comprehensive transformations.\"\"\"\n",
        "    logger.info(\"Processing customers data\")\n",
        "    \n",
        "    # Data validation\n",
        "    validate_data_quality(df, \"customers\")\n",
        "    \n",
        "    # Data cleaning - standardize text fields\n",
        "    cleaned_df = df.withColumn(\"FirstName\", initcap(trim(col(\"FirstName\")))) \\\n",
        "                 .withColumn(\"LastName\", initcap(trim(col(\"LastName\")))) \\\n",
        "                 .withColumn(\"Email\", lower(trim(col(\"Email\")))) \\\n",
        "                 .withColumn(\"City\", initcap(trim(col(\"City\")))) \\\n",
        "                 .withColumn(\"State\", upper(trim(col(\"State\")))) \\\n",
        "                 .withColumn(\"Country\", initcap(trim(col(\"Country\"))))\n",
        "    \n",
        "    # Create derived fields for Silver layer\n",
        "    enriched_df = cleaned_df.withColumn(\"FullName\", \n",
        "                                       concat_ws(\" \", col(\"FirstName\"), col(\"LastName\"))) \\\n",
        "                           .withColumn(\"CustomerAge\", \n",
        "                                       round(datediff(current_date(), col(\"SignupDate\")) / 365.25, 0)) \\\n",
        "                           .withColumn(\"CustomerSegment\", \n",
        "                                       when(col(\"SignupDate\") >= \"2024-01-01\", \"New\")\n",
        "                                       .when(col(\"SignupDate\") >= \"2023-01-01\", \"Recent\")\n",
        "                                       .otherwise(\"Established\")) \\\n",
        "                           .withColumn(\"CustomerTier\", \n",
        "                                       when(col(\"SignupDate\") >= \"2024-01-01\", \"Premium\")\n",
        "                                       .when(col(\"SignupDate\") >= \"2023-06-01\", \"Gold\")\n",
        "                                       .otherwise(\"Standard\"))\n",
        "    \n",
        "    # Data quality checks\n",
        "    invalid_emails = enriched_df.filter(~col(\"Email\").rlike(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$')).count()\n",
        "    if invalid_emails > 0:\n",
        "        logger.warning(f\"Found {invalid_emails} invalid email addresses\")\n",
        "    \n",
        "    return enriched_df\n",
        "\n",
        "def clean_and_validate_accounts(df):\n",
        "    \"\"\"Clean and validate account data with comprehensive transformations.\"\"\"\n",
        "    logger.info(\"Processing accounts data\")\n",
        "    \n",
        "    # Data validation\n",
        "    validate_data_quality(df, \"accounts\")\n",
        "    \n",
        "    # Data cleaning - standardize account types and validate balances\n",
        "    cleaned_df = df.withColumn(\"AccountType\", \n",
        "                              when(col(\"AccountType\").isin([\"Savings\", \"Saving\"]), \"Savings\")\n",
        "                              .when(col(\"AccountType\").isin([\"Checking\", \"Check\"]), \"Checking\")\n",
        "                              .when(col(\"AccountType\").isin([\"Investment\", \"Invest\"]), \"Investment\")\n",
        "                              .when(col(\"AccountType\").isin([\"Business\", \"Biz\"]), \"Business\")\n",
        "                              .otherwise(col(\"AccountType\"))) \\\n",
        "                  .withColumn(\"Balance\", \n",
        "                              when(col(\"Balance\") < 0, 0.0).otherwise(col(\"Balance\")))\n",
        "    \n",
        "    # Create derived fields for Silver layer\n",
        "    enriched_df = cleaned_df.withColumn(\"AccountAgeYears\", \n",
        "                                       round(datediff(current_date(), col(\"OpenDate\")) / 365.25, 2)) \\\n",
        "                           .withColumn(\"AccountStatus\", \n",
        "                                       when(col(\"Balance\") == 0, \"Inactive\")\n",
        "                                       .when(col(\"Balance\") < 1000, \"Low Balance\")\n",
        "                                       .when(col(\"Balance\") < 10000, \"Standard\")\n",
        "                                       .otherwise(\"Premium\")) \\\n",
        "                           .withColumn(\"AccountTier\", \n",
        "                                       when(col(\"Balance\") >= 50000, \"Gold\")\n",
        "                                       .when(col(\"Balance\") >= 25000, \"Silver\")\n",
        "                                       .when(col(\"Balance\") >= 10000, \"Bronze\")\n",
        "                                       .otherwise(\"Basic\")) \\\n",
        "                           .withColumn(\"IsHighValue\", \n",
        "                                       col(\"Balance\") >= 50000)\n",
        "    \n",
        "    # Data quality checks\n",
        "    negative_balances = enriched_df.filter(col(\"Balance\") < 0).count()\n",
        "    if negative_balances > 0:\n",
        "        logger.warning(f\"Found {negative_balances} accounts with negative balances\")\n",
        "    \n",
        "    return enriched_df\n",
        "\n",
        "def clean_and_validate_loans(df):\n",
        "    \"\"\"Clean and validate loan data with comprehensive transformations.\"\"\"\n",
        "    logger.info(\"Processing loans data\")\n",
        "    \n",
        "    # Data validation\n",
        "    validate_data_quality(df, \"loans\")\n",
        "    \n",
        "    # Data cleaning - standardize loan types and validate amounts\n",
        "    cleaned_df = df.withColumn(\"LoanType\", \n",
        "                              when(col(\"LoanType\").isin([\"Personal\", \"Personal Loan\"]), \"Personal\")\n",
        "                              .when(col(\"LoanType\").isin([\"Home\", \"Home Loan\", \"Mortgage\"]), \"Home\")\n",
        "                              .when(col(\"LoanType\").isin([\"Car\", \"Auto\", \"Car Loan\"]), \"Car\")\n",
        "                              .when(col(\"LoanType\").isin([\"Education\", \"Student\", \"Education Loan\"]), \"Education\")\n",
        "                              .when(col(\"LoanType\").isin([\"Business\", \"Business Loan\"]), \"Business\")\n",
        "                              .otherwise(col(\"LoanType\"))) \\\n",
        "                  .withColumn(\"LoanAmount\", \n",
        "                              when(col(\"LoanAmount\") <= 0, 0.0).otherwise(col(\"LoanAmount\"))) \\\n",
        "                  .withColumn(\"InterestRate\", \n",
        "                              when(col(\"InterestRate\") < 0, 0.0)\n",
        "                              .when(col(\"InterestRate\") > 30.0, 30.0)\n",
        "                              .otherwise(col(\"InterestRate\")))\n",
        "    \n",
        "    # Create derived fields for Silver layer\n",
        "    enriched_df = cleaned_df.withColumn(\"LoanDurationYears\", \n",
        "                                       round(datediff(col(\"LoanEndDate\"), col(\"LoanStartDate\")) / 365.25, 2)) \\\n",
        "                           .withColumn(\"TotalInterest\", \n",
        "                                       (col(\"LoanAmount\") * col(\"InterestRate\") / 100).cast(DecimalType(18, 2))) \\\n",
        "                           .withColumn(\"MonthlyPayment\", \n",
        "                                       ((col(\"LoanAmount\") + col(\"TotalInterest\")) / (col(\"LoanDurationYears\") * 12)).cast(DecimalType(18, 2))) \\\n",
        "                           .withColumn(\"LoanStatus\", \n",
        "                                       when(col(\"LoanEndDate\") < current_date(), \"Completed\")\n",
        "                                       .when(col(\"LoanStartDate\") > current_date(), \"Pending\")\n",
        "                                       .otherwise(\"Active\")) \\\n",
        "                           .withColumn(\"RiskCategory\", \n",
        "                                       when((col(\"LoanAmount\") > 50000) & (col(\"InterestRate\") > 10), \"High Risk\")\n",
        "                                       .when((col(\"LoanAmount\") > 25000) & (col(\"InterestRate\") > 7), \"Medium Risk\")\n",
        "                                       .otherwise(\"Low Risk\")) \\\n",
        "                           .withColumn(\"LoanToValueRatio\", \n",
        "                                       (col(\"LoanAmount\") / 100000).cast(DecimalType(5, 2))) \\\n",
        "                           .withColumn(\"IsHighRisk\", \n",
        "                                       (col(\"LoanAmount\") > 50000) & (col(\"InterestRate\") > 10))\n",
        "    \n",
        "    # Data quality checks\n",
        "    invalid_dates = enriched_df.filter(col(\"LoanStartDate\") >= col(\"LoanEndDate\")).count()\n",
        "    if invalid_dates > 0:\n",
        "        logger.warning(f\"Found {invalid_dates} loans with invalid date ranges\")\n",
        "    \n",
        "    return enriched_df\n",
        "\n",
        "def clean_and_validate_transactions(df):\n",
        "    \"\"\"Clean and validate transaction data with comprehensive transformations.\"\"\"\n",
        "    logger.info(\"Processing transactions data\")\n",
        "    \n",
        "    # Data validation\n",
        "    validate_data_quality(df, \"transactions\")\n",
        "    \n",
        "    # Data cleaning - standardize transaction types and validate amounts\n",
        "    cleaned_df = df.withColumn(\"TransactionType\", \n",
        "                              when(col(\"TransactionType\").isin([\"Deposit\", \"Credit\"]), \"Deposit\")\n",
        "                              .when(col(\"TransactionType\").isin([\"Withdrawal\", \"Debit\"]), \"Withdrawal\")\n",
        "                              .when(col(\"TransactionType\").isin([\"Transfer\", \"Transfer In\", \"Transfer Out\"]), \"Transfer\")\n",
        "                              .when(col(\"TransactionType\").isin([\"Fee\", \"Service Fee\", \"Maintenance Fee\"]), \"Fee\")\n",
        "                              .otherwise(col(\"TransactionType\"))) \\\n",
        "                  .withColumn(\"Amount\", \n",
        "                              when(col(\"Amount\") <= 0, 0.01).otherwise(col(\"Amount\"))) \\\n",
        "                  .withColumn(\"Description\", trim(col(\"Description\")))\n",
        "    \n",
        "    # Create derived fields for Silver layer\n",
        "    enriched_df = cleaned_df.withColumn(\"TransactionCategory\", \n",
        "                                       when(col(\"TransactionType\") == \"Deposit\", \"Income\")\n",
        "                                       .when(col(\"TransactionType\") == \"Withdrawal\", \"Expense\")\n",
        "                                       .when(col(\"TransactionType\") == \"Transfer\", \"Transfer\")\n",
        "                                       .when(col(\"TransactionType\") == \"Fee\", \"Fee\")\n",
        "                                       .otherwise(\"Other\")) \\\n",
        "                           .withColumn(\"TransactionSize\", \n",
        "                                       when(col(\"Amount\") >= 10000, \"Large\")\n",
        "                                       .when(col(\"Amount\") >= 1000, \"Medium\")\n",
        "                                       .when(col(\"Amount\") >= 100, \"Small\")\n",
        "                                       .otherwise(\"Micro\")) \\\n",
        "                           .withColumn(\"TransactionDayOfWeek\", \n",
        "                                       date_format(col(\"TransactionDate\"), \"EEEE\")) \\\n",
        "                           .withColumn(\"TransactionMonth\", \n",
        "                                       date_format(col(\"TransactionDate\"), \"MM\")) \\\n",
        "                           .withColumn(\"TransactionYear\", \n",
        "                                       date_format(col(\"TransactionDate\"), \"yyyy\")) \\\n",
        "                           .withColumn(\"IsWeekend\", \n",
        "                                       when(col(\"TransactionDayOfWeek\").isin([\"Saturday\", \"Sunday\"]), True)\n",
        "                                       .otherwise(False)) \\\n",
        "                           .withColumn(\"IsLargeTransaction\", \n",
        "                                       col(\"Amount\") >= 10000)\n",
        "    \n",
        "    return enriched_df\n",
        "\n",
        "def clean_and_validate_payments(df):\n",
        "    \"\"\"Clean and validate payment data with comprehensive transformations.\"\"\"\n",
        "    logger.info(\"Processing payments data\")\n",
        "    \n",
        "    # Data validation\n",
        "    validate_data_quality(df, \"payments\")\n",
        "    \n",
        "    # Data cleaning - standardize payment methods and validate amounts\n",
        "    cleaned_df = df.withColumn(\"PaymentMethod\", \n",
        "                              when(col(\"PaymentMethod\").isin([\"Credit Card\", \"Credit\"]), \"Credit Card\")\n",
        "                              .when(col(\"PaymentMethod\").isin([\"Debit Card\", \"Debit\"]), \"Debit Card\")\n",
        "                              .when(col(\"PaymentMethod\").isin([\"Cash\", \"Cash Payment\"]), \"Cash\")\n",
        "                              .when(col(\"PaymentMethod\").isin([\"Bank Transfer\", \"Wire Transfer\", \"ACH\"]), \"Bank Transfer\")\n",
        "                              .when(col(\"PaymentMethod\").isin([\"Check\", \"Cheque\"]), \"Check\")\n",
        "                              .otherwise(col(\"PaymentMethod\"))) \\\n",
        "                  .withColumn(\"PaymentAmount\", \n",
        "                              when(col(\"PaymentAmount\") <= 0, 0.01).otherwise(col(\"PaymentAmount\")))\n",
        "    \n",
        "    # Create derived fields for Silver layer\n",
        "    enriched_df = cleaned_df.withColumn(\"DaysSinceLastPayment\", \n",
        "                                       datediff(current_date(), col(\"PaymentDate\"))) \\\n",
        "                           .withColumn(\"PaymentSize\", \n",
        "                                       when(col(\"PaymentAmount\") >= 5000, \"Large\")\n",
        "                                       .when(col(\"PaymentAmount\") >= 1000, \"Medium\")\n",
        "                                       .when(col(\"PaymentAmount\") >= 100, \"Small\")\n",
        "                                       .otherwise(\"Micro\")) \\\n",
        "                           .withColumn(\"PaymentMethodCategory\", \n",
        "                                       when(col(\"PaymentMethod\").isin([\"Credit Card\", \"Debit Card\"]), \"Card\")\n",
        "                                       .when(col(\"PaymentMethod\") == \"Cash\", \"Cash\")\n",
        "                                       .when(col(\"PaymentMethod\") == \"Bank Transfer\", \"Electronic\")\n",
        "                                       .when(col(\"PaymentMethod\") == \"Check\", \"Check\")\n",
        "                                       .otherwise(\"Other\")) \\\n",
        "                           .withColumn(\"IsLatePayment\", \n",
        "                                       col(\"DaysSinceLastPayment\") > 30) \\\n",
        "                           .withColumn(\"IsLargePayment\", \n",
        "                                       col(\"PaymentAmount\") >= 5000)\n",
        "    \n",
        "    return enriched_df\n",
        "\n",
        "# Process each table with improved transformations\n",
        "try:\n",
        "    # Process Customers\n",
        "    customers_df = spark.read.parquet(f\"{base_path}Customers/Customers.parquet\")\n",
        "    customers_processed = clean_and_validate_customers(customers_df)\n",
        "    customers_processed.write.format(\"delta\").mode(\"overwrite\").save(f\"{output_base_path}Customers/\")\n",
        "    logger.info(\"Customers processing completed\")\n",
        "    \n",
        "    # Process Accounts\n",
        "    accounts_df = spark.read.parquet(f\"{base_path}Accounts/Accounts.parquet\")\n",
        "    accounts_processed = clean_and_validate_accounts(accounts_df)\n",
        "    accounts_processed.write.format(\"delta\").mode(\"overwrite\").save(f\"{output_base_path}Accounts/\")\n",
        "    logger.info(\"Accounts processing completed\")\n",
        "    \n",
        "    # Process Loans\n",
        "    loans_df = spark.read.parquet(f\"{base_path}Loans/Loans.parquet\")\n",
        "    loans_processed = clean_and_validate_loans(loans_df)\n",
        "    loans_processed.write.format(\"delta\").mode(\"overwrite\").save(f\"{output_base_path}Loans/\")\n",
        "    logger.info(\"Loans processing completed\")\n",
        "    \n",
        "    # Process Transactions\n",
        "    transactions_df = spark.read.parquet(f\"{base_path}Transactions/Transactions.parquet\")\n",
        "    transactions_processed = clean_and_validate_transactions(transactions_df)\n",
        "    transactions_processed.write.format(\"delta\").mode(\"overwrite\").save(f\"{output_base_path}Transactions/\")\n",
        "    logger.info(\"Transactions processing completed\")\n",
        "    \n",
        "    # Process Payments\n",
        "    payments_df = spark.read.parquet(f\"{base_path}Payments/Payments.parquet\")\n",
        "    payments_processed = clean_and_validate_payments(payments_df)\n",
        "    payments_processed.write.format(\"delta\").mode(\"overwrite\").save(f\"{output_base_path}Payments/\")\n",
        "    logger.info(\"Payments processing completed\")\n",
        "    \n",
        "    print(\"Bronze To Silver Processing Completed Successfully!\")\n",
        "    logger.info(\"All tables processed with comprehensive data validation and cleaning\")\n",
        "    \n",
        "except Exception as e:\n",
        "    logger.error(f\"Error in Bronze to Silver processing: {str(e)}\")\n",
        "    print(f\"Error: {str(e)}\")\n",
        "    raise\n",
        ""
      ]
    }
  ]
}